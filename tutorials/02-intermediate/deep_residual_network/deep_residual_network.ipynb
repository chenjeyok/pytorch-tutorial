{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configurations\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters\n",
    "num_epochs = 2\n",
    "num_classes= 10\n",
    "learning_rate = 0.001\n",
    "batch_size=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Processing Modules\n",
    "transform = transforms.Compose([\n",
    "    transforms.Pad(4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32),\n",
    "    transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CIFAR-10 dataset\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='../../data/',\n",
    "                                             train=True, \n",
    "                                             transform=transform,  # using transform when training while no transform when testing\n",
    "                                             download=False)\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='../../data/',\n",
    "                                            train=False, \n",
    "                                            transform=transforms.ToTensor())\n",
    "\n",
    "# Data loader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A frequently used conv\n",
    "def conv3x3(in_channels, out_channels, stride=1):\n",
    "    return nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, stride=stride, padding=1, bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ResNet's famous ResidualBlock\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(in_channels=in_channels, out_channels=out_channels, stride=stride)\n",
    "        self.bn1= nn.BatchNorm2d(num_features=out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(in_channels=out_channels, out_channels=out_channels)\n",
    "        self.bn2 = nn.BatchNorm2d(num_features=out_channels)\n",
    "        self.downsample = downsample\n",
    "    \n",
    "    def forward(self, x): # if this Residual Block has downsampled the inputs\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        if self.downsample:\n",
    "            residual = self.downsample(x)\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ResNet\n",
    "class ResNet(nn.Module):\n",
    "    \n",
    "    # ResNet is actually composed of ResidualBlocks, whose class prototype is passed in as \"block\"\n",
    "    # a layer is consist of ResidualBlocks (layers[k] are num of ResidualBlocks of layer k)\n",
    "    def __init__(self, block, layers, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_channels = 16\n",
    "        self.conv = conv3x3(in_channels=3, out_channels=16)\n",
    "        self.bn = nn.BatchNorm2d(num_features=16)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.layer1 = self.make_layer(block=block, out_channels=16, blocks=layers[0], stride=1)\n",
    "        self.layer2 = self.make_layer(block=block, out_channels=32, blocks=layers[0], stride=2)\n",
    "        self.layer3 = self.make_layer(block=block, out_channels=64, blocks=layers[1], stride=2)\n",
    "        self.avg_pool = nn.AvgPool2d(kernel_size=8)\n",
    "        self.fc = nn.Linear(in_features=64, out_features=num_classes)\n",
    "        \n",
    "    def make_layer(self, block, out_channels, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if (stride!=1) or (self.in_channels != out_channels):\n",
    "            downsample = nn.Sequential(\n",
    "                conv3x3(in_channels=self.in_channels, out_channels=out_channels, stride=stride),\n",
    "                nn.BatchNorm2d(out_channels))\n",
    "        \n",
    "        layers = []\n",
    "        layers.append(block(self.in_channels, out_channels, stride, downsample))\n",
    "        self.in_channels = out_channels\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(out_channels, out_channels))\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.conv(x)\n",
    "        out = self.bn(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.avg_pool(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init Net & Loss Func & Optimizer\n",
    "model = ResNet(ResidualBlock, [2,2,2,2]).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "for param in criterion.parameters():\n",
    "    print(param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for updating learning rate\n",
    "def update_lr(optimizer, lr):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2], Step [1/500] Loss: 2.3178\n",
      "Epoch [1/2], Step [2/500] Loss: 2.2689\n",
      "Epoch [1/2], Step [3/500] Loss: 2.2506\n",
      "Epoch [1/2], Step [4/500] Loss: 2.1893\n",
      "Epoch [1/2], Step [5/500] Loss: 2.1937\n",
      "Epoch [1/2], Step [6/500] Loss: 2.1620\n",
      "Epoch [1/2], Step [7/500] Loss: 2.1337\n",
      "Epoch [1/2], Step [8/500] Loss: 2.0788\n",
      "Epoch [1/2], Step [9/500] Loss: 2.0407\n",
      "Epoch [1/2], Step [10/500] Loss: 2.0308\n",
      "Epoch [1/2], Step [11/500] Loss: 2.0681\n",
      "Epoch [1/2], Step [12/500] Loss: 1.9786\n",
      "Epoch [1/2], Step [13/500] Loss: 1.9680\n",
      "Epoch [1/2], Step [14/500] Loss: 2.0013\n",
      "Epoch [1/2], Step [15/500] Loss: 1.9849\n",
      "Epoch [1/2], Step [16/500] Loss: 1.8921\n",
      "Epoch [1/2], Step [17/500] Loss: 1.9099\n",
      "Epoch [1/2], Step [18/500] Loss: 1.9642\n",
      "Epoch [1/2], Step [19/500] Loss: 1.9957\n",
      "Epoch [1/2], Step [20/500] Loss: 1.9018\n",
      "Epoch [1/2], Step [21/500] Loss: 1.9149\n",
      "Epoch [1/2], Step [22/500] Loss: 1.9803\n",
      "Epoch [1/2], Step [23/500] Loss: 2.0222\n",
      "Epoch [1/2], Step [24/500] Loss: 1.9821\n",
      "Epoch [1/2], Step [25/500] Loss: 1.9576\n",
      "Epoch [1/2], Step [26/500] Loss: 1.8764\n",
      "Epoch [1/2], Step [27/500] Loss: 2.0037\n",
      "Epoch [1/2], Step [28/500] Loss: 1.9512\n",
      "Epoch [1/2], Step [29/500] Loss: 1.8197\n",
      "Epoch [1/2], Step [30/500] Loss: 1.9684\n",
      "Epoch [1/2], Step [31/500] Loss: 1.9104\n",
      "Epoch [1/2], Step [32/500] Loss: 1.8711\n",
      "Epoch [1/2], Step [33/500] Loss: 1.9358\n",
      "Epoch [1/2], Step [34/500] Loss: 1.7963\n",
      "Epoch [1/2], Step [35/500] Loss: 1.9810\n",
      "Epoch [1/2], Step [36/500] Loss: 1.8168\n",
      "Epoch [1/2], Step [37/500] Loss: 1.8148\n",
      "Epoch [1/2], Step [38/500] Loss: 1.8918\n",
      "Epoch [1/2], Step [39/500] Loss: 1.9105\n",
      "Epoch [1/2], Step [40/500] Loss: 1.8140\n",
      "Epoch [1/2], Step [41/500] Loss: 1.8087\n",
      "Epoch [1/2], Step [42/500] Loss: 1.7993\n",
      "Epoch [1/2], Step [43/500] Loss: 2.0583\n",
      "Epoch [1/2], Step [44/500] Loss: 1.9151\n",
      "Epoch [1/2], Step [45/500] Loss: 1.7969\n",
      "Epoch [1/2], Step [46/500] Loss: 1.8344\n",
      "Epoch [1/2], Step [47/500] Loss: 1.9124\n",
      "Epoch [1/2], Step [48/500] Loss: 1.7964\n",
      "Epoch [1/2], Step [49/500] Loss: 1.9069\n",
      "Epoch [1/2], Step [50/500] Loss: 1.8565\n",
      "Epoch [1/2], Step [51/500] Loss: 1.7592\n",
      "Epoch [1/2], Step [52/500] Loss: 1.8548\n",
      "Epoch [1/2], Step [53/500] Loss: 1.7549\n",
      "Epoch [1/2], Step [54/500] Loss: 1.7535\n",
      "Epoch [1/2], Step [55/500] Loss: 1.8616\n",
      "Epoch [1/2], Step [56/500] Loss: 1.8127\n",
      "Epoch [1/2], Step [57/500] Loss: 1.8539\n",
      "Epoch [1/2], Step [58/500] Loss: 1.7870\n",
      "Epoch [1/2], Step [59/500] Loss: 1.8476\n",
      "Epoch [1/2], Step [60/500] Loss: 1.7052\n",
      "Epoch [1/2], Step [61/500] Loss: 1.8218\n",
      "Epoch [1/2], Step [62/500] Loss: 1.7094\n",
      "Epoch [1/2], Step [63/500] Loss: 1.7350\n",
      "Epoch [1/2], Step [64/500] Loss: 1.6857\n",
      "Epoch [1/2], Step [65/500] Loss: 1.7441\n",
      "Epoch [1/2], Step [66/500] Loss: 1.7223\n",
      "Epoch [1/2], Step [67/500] Loss: 1.7091\n",
      "Epoch [1/2], Step [68/500] Loss: 1.7884\n",
      "Epoch [1/2], Step [69/500] Loss: 1.6679\n",
      "Epoch [1/2], Step [70/500] Loss: 1.7655\n",
      "Epoch [1/2], Step [71/500] Loss: 1.7407\n",
      "Epoch [1/2], Step [72/500] Loss: 1.7619\n",
      "Epoch [1/2], Step [73/500] Loss: 1.6220\n",
      "Epoch [1/2], Step [74/500] Loss: 1.6747\n",
      "Epoch [1/2], Step [75/500] Loss: 1.8170\n",
      "Epoch [1/2], Step [76/500] Loss: 1.6852\n",
      "Epoch [1/2], Step [77/500] Loss: 1.6777\n",
      "Epoch [1/2], Step [78/500] Loss: 1.6050\n",
      "Epoch [1/2], Step [79/500] Loss: 1.7166\n",
      "Epoch [1/2], Step [80/500] Loss: 1.6912\n",
      "Epoch [1/2], Step [81/500] Loss: 1.6333\n",
      "Epoch [1/2], Step [82/500] Loss: 1.7790\n",
      "Epoch [1/2], Step [83/500] Loss: 1.6936\n",
      "Epoch [1/2], Step [84/500] Loss: 1.5831\n",
      "Epoch [1/2], Step [85/500] Loss: 1.8056\n",
      "Epoch [1/2], Step [86/500] Loss: 1.7795\n",
      "Epoch [1/2], Step [87/500] Loss: 1.5623\n",
      "Epoch [1/2], Step [88/500] Loss: 1.6552\n",
      "Epoch [1/2], Step [89/500] Loss: 1.6988\n",
      "Epoch [1/2], Step [90/500] Loss: 1.5676\n",
      "Epoch [1/2], Step [91/500] Loss: 1.6593\n",
      "Epoch [1/2], Step [92/500] Loss: 1.7620\n",
      "Epoch [1/2], Step [93/500] Loss: 1.8314\n",
      "Epoch [1/2], Step [94/500] Loss: 1.8062\n",
      "Epoch [1/2], Step [95/500] Loss: 1.8138\n",
      "Epoch [1/2], Step [96/500] Loss: 1.6790\n",
      "Epoch [1/2], Step [97/500] Loss: 1.7034\n",
      "Epoch [1/2], Step [98/500] Loss: 1.7096\n",
      "Epoch [1/2], Step [99/500] Loss: 1.6654\n",
      "Epoch [1/2], Step [100/500] Loss: 1.7092\n",
      "Epoch [1/2], Step [101/500] Loss: 1.7603\n",
      "Epoch [1/2], Step [102/500] Loss: 1.5814\n",
      "Epoch [1/2], Step [103/500] Loss: 1.6905\n",
      "Epoch [1/2], Step [104/500] Loss: 1.5802\n",
      "Epoch [1/2], Step [105/500] Loss: 1.6925\n",
      "Epoch [1/2], Step [106/500] Loss: 1.6625\n",
      "Epoch [1/2], Step [107/500] Loss: 1.6272\n",
      "Epoch [1/2], Step [108/500] Loss: 1.5717\n",
      "Epoch [1/2], Step [109/500] Loss: 1.7550\n",
      "Epoch [1/2], Step [110/500] Loss: 1.6718\n",
      "Epoch [1/2], Step [111/500] Loss: 1.6498\n",
      "Epoch [1/2], Step [112/500] Loss: 1.7204\n",
      "Epoch [1/2], Step [113/500] Loss: 1.4855\n",
      "Epoch [1/2], Step [114/500] Loss: 1.6225\n",
      "Epoch [1/2], Step [115/500] Loss: 1.5999\n",
      "Epoch [1/2], Step [116/500] Loss: 1.5904\n",
      "Epoch [1/2], Step [117/500] Loss: 1.6159\n",
      "Epoch [1/2], Step [118/500] Loss: 1.6526\n",
      "Epoch [1/2], Step [119/500] Loss: 1.6004\n",
      "Epoch [1/2], Step [120/500] Loss: 1.5168\n",
      "Epoch [1/2], Step [121/500] Loss: 1.5967\n",
      "Epoch [1/2], Step [122/500] Loss: 1.5717\n",
      "Epoch [1/2], Step [123/500] Loss: 1.6780\n",
      "Epoch [1/2], Step [124/500] Loss: 1.7372\n",
      "Epoch [1/2], Step [125/500] Loss: 1.5764\n",
      "Epoch [1/2], Step [126/500] Loss: 1.5177\n",
      "Epoch [1/2], Step [127/500] Loss: 1.4799\n",
      "Epoch [1/2], Step [128/500] Loss: 1.6580\n",
      "Epoch [1/2], Step [129/500] Loss: 1.5549\n",
      "Epoch [1/2], Step [130/500] Loss: 1.6327\n",
      "Epoch [1/2], Step [131/500] Loss: 1.5756\n",
      "Epoch [1/2], Step [132/500] Loss: 1.6226\n",
      "Epoch [1/2], Step [133/500] Loss: 1.6906\n",
      "Epoch [1/2], Step [134/500] Loss: 1.5629\n",
      "Epoch [1/2], Step [135/500] Loss: 1.5275\n",
      "Epoch [1/2], Step [136/500] Loss: 1.6066\n",
      "Epoch [1/2], Step [137/500] Loss: 1.5822\n",
      "Epoch [1/2], Step [138/500] Loss: 1.5031\n",
      "Epoch [1/2], Step [139/500] Loss: 1.6766\n",
      "Epoch [1/2], Step [140/500] Loss: 1.7437\n",
      "Epoch [1/2], Step [141/500] Loss: 1.5870\n",
      "Epoch [1/2], Step [142/500] Loss: 1.4801\n",
      "Epoch [1/2], Step [143/500] Loss: 1.6039\n",
      "Epoch [1/2], Step [144/500] Loss: 1.6352\n",
      "Epoch [1/2], Step [145/500] Loss: 1.4845\n",
      "Epoch [1/2], Step [146/500] Loss: 1.6807\n",
      "Epoch [1/2], Step [147/500] Loss: 1.5367\n",
      "Epoch [1/2], Step [148/500] Loss: 1.5364\n",
      "Epoch [1/2], Step [149/500] Loss: 1.6831\n",
      "Epoch [1/2], Step [150/500] Loss: 1.7511\n",
      "Epoch [1/2], Step [151/500] Loss: 1.5266\n",
      "Epoch [1/2], Step [152/500] Loss: 1.5301\n",
      "Epoch [1/2], Step [153/500] Loss: 1.5189\n",
      "Epoch [1/2], Step [154/500] Loss: 1.6704\n",
      "Epoch [1/2], Step [155/500] Loss: 1.5785\n",
      "Epoch [1/2], Step [156/500] Loss: 1.5175\n",
      "Epoch [1/2], Step [157/500] Loss: 1.4894\n",
      "Epoch [1/2], Step [158/500] Loss: 1.5861\n",
      "Epoch [1/2], Step [159/500] Loss: 1.5197\n",
      "Epoch [1/2], Step [160/500] Loss: 1.6882\n",
      "Epoch [1/2], Step [161/500] Loss: 1.5012\n",
      "Epoch [1/2], Step [162/500] Loss: 1.6235\n",
      "Epoch [1/2], Step [163/500] Loss: 1.5940\n",
      "Epoch [1/2], Step [164/500] Loss: 1.6422\n",
      "Epoch [1/2], Step [165/500] Loss: 1.5397\n",
      "Epoch [1/2], Step [166/500] Loss: 1.6819\n",
      "Epoch [1/2], Step [167/500] Loss: 1.3885\n",
      "Epoch [1/2], Step [168/500] Loss: 1.6259\n",
      "Epoch [1/2], Step [169/500] Loss: 1.5181\n",
      "Epoch [1/2], Step [170/500] Loss: 1.7173\n",
      "Epoch [1/2], Step [171/500] Loss: 1.6386\n",
      "Epoch [1/2], Step [172/500] Loss: 1.5177\n",
      "Epoch [1/2], Step [173/500] Loss: 1.4562\n",
      "Epoch [1/2], Step [174/500] Loss: 1.4940\n",
      "Epoch [1/2], Step [175/500] Loss: 1.4075\n",
      "Epoch [1/2], Step [176/500] Loss: 1.6956\n",
      "Epoch [1/2], Step [177/500] Loss: 1.4579\n",
      "Epoch [1/2], Step [178/500] Loss: 1.3437\n",
      "Epoch [1/2], Step [179/500] Loss: 1.3982\n",
      "Epoch [1/2], Step [180/500] Loss: 1.4938\n",
      "Epoch [1/2], Step [181/500] Loss: 1.6546\n",
      "Epoch [1/2], Step [182/500] Loss: 1.6288\n",
      "Epoch [1/2], Step [183/500] Loss: 1.6774\n",
      "Epoch [1/2], Step [184/500] Loss: 1.4786\n",
      "Epoch [1/2], Step [185/500] Loss: 1.6415\n",
      "Epoch [1/2], Step [186/500] Loss: 1.5342\n",
      "Epoch [1/2], Step [187/500] Loss: 1.4586\n",
      "Epoch [1/2], Step [188/500] Loss: 1.6380\n",
      "Epoch [1/2], Step [189/500] Loss: 1.5168\n",
      "Epoch [1/2], Step [190/500] Loss: 1.6729\n",
      "Epoch [1/2], Step [191/500] Loss: 1.5715\n",
      "Epoch [1/2], Step [192/500] Loss: 1.4274\n",
      "Epoch [1/2], Step [193/500] Loss: 1.5673\n",
      "Epoch [1/2], Step [194/500] Loss: 1.4418\n",
      "Epoch [1/2], Step [195/500] Loss: 1.4542\n",
      "Epoch [1/2], Step [196/500] Loss: 1.5247\n",
      "Epoch [1/2], Step [197/500] Loss: 1.5138\n",
      "Epoch [1/2], Step [198/500] Loss: 1.4473\n",
      "Epoch [1/2], Step [199/500] Loss: 1.4415\n",
      "Epoch [1/2], Step [200/500] Loss: 1.4063\n",
      "Epoch [1/2], Step [201/500] Loss: 1.4279\n",
      "Epoch [1/2], Step [202/500] Loss: 1.5387\n",
      "Epoch [1/2], Step [203/500] Loss: 1.4625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2], Step [204/500] Loss: 1.5780\n",
      "Epoch [1/2], Step [205/500] Loss: 1.4807\n",
      "Epoch [1/2], Step [206/500] Loss: 1.4424\n",
      "Epoch [1/2], Step [207/500] Loss: 1.4919\n",
      "Epoch [1/2], Step [208/500] Loss: 1.4198\n",
      "Epoch [1/2], Step [209/500] Loss: 1.5606\n",
      "Epoch [1/2], Step [210/500] Loss: 1.3574\n",
      "Epoch [1/2], Step [211/500] Loss: 1.4911\n",
      "Epoch [1/2], Step [212/500] Loss: 1.3797\n",
      "Epoch [1/2], Step [213/500] Loss: 1.4822\n",
      "Epoch [1/2], Step [214/500] Loss: 1.4021\n",
      "Epoch [1/2], Step [215/500] Loss: 1.5840\n",
      "Epoch [1/2], Step [216/500] Loss: 1.3966\n",
      "Epoch [1/2], Step [217/500] Loss: 1.4545\n",
      "Epoch [1/2], Step [218/500] Loss: 1.4777\n",
      "Epoch [1/2], Step [219/500] Loss: 1.4694\n",
      "Epoch [1/2], Step [220/500] Loss: 1.3869\n",
      "Epoch [1/2], Step [221/500] Loss: 1.4096\n",
      "Epoch [1/2], Step [222/500] Loss: 1.5131\n",
      "Epoch [1/2], Step [223/500] Loss: 1.4397\n",
      "Epoch [1/2], Step [224/500] Loss: 1.3971\n",
      "Epoch [1/2], Step [225/500] Loss: 1.2019\n",
      "Epoch [1/2], Step [226/500] Loss: 1.4513\n",
      "Epoch [1/2], Step [227/500] Loss: 1.5105\n",
      "Epoch [1/2], Step [228/500] Loss: 1.2741\n",
      "Epoch [1/2], Step [229/500] Loss: 1.3631\n",
      "Epoch [1/2], Step [230/500] Loss: 1.6192\n",
      "Epoch [1/2], Step [231/500] Loss: 1.4218\n",
      "Epoch [1/2], Step [232/500] Loss: 1.3918\n",
      "Epoch [1/2], Step [233/500] Loss: 1.3956\n",
      "Epoch [1/2], Step [234/500] Loss: 1.5052\n",
      "Epoch [1/2], Step [235/500] Loss: 1.3615\n",
      "Epoch [1/2], Step [236/500] Loss: 1.4429\n",
      "Epoch [1/2], Step [237/500] Loss: 1.5090\n",
      "Epoch [1/2], Step [238/500] Loss: 1.3710\n",
      "Epoch [1/2], Step [239/500] Loss: 1.4532\n",
      "Epoch [1/2], Step [240/500] Loss: 1.4165\n",
      "Epoch [1/2], Step [241/500] Loss: 1.4072\n",
      "Epoch [1/2], Step [242/500] Loss: 1.4733\n",
      "Epoch [1/2], Step [243/500] Loss: 1.3530\n",
      "Epoch [1/2], Step [244/500] Loss: 1.5317\n",
      "Epoch [1/2], Step [245/500] Loss: 1.4615\n",
      "Epoch [1/2], Step [246/500] Loss: 1.5015\n",
      "Epoch [1/2], Step [247/500] Loss: 1.4714\n",
      "Epoch [1/2], Step [248/500] Loss: 1.5026\n",
      "Epoch [1/2], Step [249/500] Loss: 1.2992\n",
      "Epoch [1/2], Step [250/500] Loss: 1.3938\n",
      "Epoch [1/2], Step [251/500] Loss: 1.4994\n",
      "Epoch [1/2], Step [252/500] Loss: 1.5676\n",
      "Epoch [1/2], Step [253/500] Loss: 1.5184\n",
      "Epoch [1/2], Step [254/500] Loss: 1.5240\n",
      "Epoch [1/2], Step [255/500] Loss: 1.4368\n",
      "Epoch [1/2], Step [256/500] Loss: 1.4069\n",
      "Epoch [1/2], Step [257/500] Loss: 1.5001\n",
      "Epoch [1/2], Step [258/500] Loss: 1.3569\n",
      "Epoch [1/2], Step [259/500] Loss: 1.3083\n",
      "Epoch [1/2], Step [260/500] Loss: 1.1987\n",
      "Epoch [1/2], Step [261/500] Loss: 1.4396\n",
      "Epoch [1/2], Step [262/500] Loss: 1.4971\n",
      "Epoch [1/2], Step [263/500] Loss: 1.3686\n",
      "Epoch [1/2], Step [264/500] Loss: 1.1499\n",
      "Epoch [1/2], Step [265/500] Loss: 1.5048\n",
      "Epoch [1/2], Step [266/500] Loss: 1.0919\n",
      "Epoch [1/2], Step [267/500] Loss: 1.3692\n",
      "Epoch [1/2], Step [268/500] Loss: 1.3617\n",
      "Epoch [1/2], Step [269/500] Loss: 1.4693\n",
      "Epoch [1/2], Step [270/500] Loss: 1.2681\n",
      "Epoch [1/2], Step [271/500] Loss: 1.3523\n",
      "Epoch [1/2], Step [272/500] Loss: 1.5075\n",
      "Epoch [1/2], Step [273/500] Loss: 1.3883\n",
      "Epoch [1/2], Step [274/500] Loss: 1.2802\n",
      "Epoch [1/2], Step [275/500] Loss: 1.4998\n",
      "Epoch [1/2], Step [276/500] Loss: 1.3002\n",
      "Epoch [1/2], Step [277/500] Loss: 1.4183\n",
      "Epoch [1/2], Step [278/500] Loss: 1.3093\n",
      "Epoch [1/2], Step [279/500] Loss: 1.1806\n",
      "Epoch [1/2], Step [280/500] Loss: 1.4067\n",
      "Epoch [1/2], Step [281/500] Loss: 1.3292\n",
      "Epoch [1/2], Step [282/500] Loss: 1.3192\n",
      "Epoch [1/2], Step [283/500] Loss: 1.5417\n",
      "Epoch [1/2], Step [284/500] Loss: 1.3783\n",
      "Epoch [1/2], Step [285/500] Loss: 1.4325\n",
      "Epoch [1/2], Step [286/500] Loss: 1.2202\n",
      "Epoch [1/2], Step [287/500] Loss: 1.4785\n",
      "Epoch [1/2], Step [288/500] Loss: 1.3556\n",
      "Epoch [1/2], Step [289/500] Loss: 1.4284\n",
      "Epoch [1/2], Step [290/500] Loss: 1.3716\n",
      "Epoch [1/2], Step [291/500] Loss: 1.2957\n",
      "Epoch [1/2], Step [292/500] Loss: 1.4734\n",
      "Epoch [1/2], Step [293/500] Loss: 1.5029\n",
      "Epoch [1/2], Step [294/500] Loss: 1.4980\n",
      "Epoch [1/2], Step [295/500] Loss: 1.2749\n",
      "Epoch [1/2], Step [296/500] Loss: 1.3401\n",
      "Epoch [1/2], Step [297/500] Loss: 1.4912\n",
      "Epoch [1/2], Step [298/500] Loss: 1.3308\n",
      "Epoch [1/2], Step [299/500] Loss: 1.3455\n",
      "Epoch [1/2], Step [300/500] Loss: 1.2825\n",
      "Epoch [1/2], Step [301/500] Loss: 1.3419\n",
      "Epoch [1/2], Step [302/500] Loss: 1.2549\n",
      "Epoch [1/2], Step [303/500] Loss: 1.3254\n",
      "Epoch [1/2], Step [304/500] Loss: 1.2968\n",
      "Epoch [1/2], Step [305/500] Loss: 1.3963\n",
      "Epoch [1/2], Step [306/500] Loss: 1.3024\n",
      "Epoch [1/2], Step [307/500] Loss: 1.3161\n",
      "Epoch [1/2], Step [308/500] Loss: 1.4109\n",
      "Epoch [1/2], Step [309/500] Loss: 1.4316\n",
      "Epoch [1/2], Step [310/500] Loss: 1.3889\n",
      "Epoch [1/2], Step [311/500] Loss: 1.3873\n",
      "Epoch [1/2], Step [312/500] Loss: 1.2580\n",
      "Epoch [1/2], Step [313/500] Loss: 1.2640\n",
      "Epoch [1/2], Step [314/500] Loss: 1.3762\n",
      "Epoch [1/2], Step [315/500] Loss: 1.3955\n",
      "Epoch [1/2], Step [316/500] Loss: 1.4384\n",
      "Epoch [1/2], Step [317/500] Loss: 1.2919\n",
      "Epoch [1/2], Step [318/500] Loss: 1.4672\n",
      "Epoch [1/2], Step [319/500] Loss: 1.4003\n",
      "Epoch [1/2], Step [320/500] Loss: 1.5284\n",
      "Epoch [1/2], Step [321/500] Loss: 1.4336\n",
      "Epoch [1/2], Step [322/500] Loss: 1.4119\n",
      "Epoch [1/2], Step [323/500] Loss: 1.2388\n",
      "Epoch [1/2], Step [324/500] Loss: 1.2603\n",
      "Epoch [1/2], Step [325/500] Loss: 1.3479\n",
      "Epoch [1/2], Step [326/500] Loss: 1.4907\n",
      "Epoch [1/2], Step [327/500] Loss: 1.3430\n",
      "Epoch [1/2], Step [328/500] Loss: 1.3144\n",
      "Epoch [1/2], Step [329/500] Loss: 1.2793\n",
      "Epoch [1/2], Step [330/500] Loss: 1.4841\n",
      "Epoch [1/2], Step [331/500] Loss: 1.2542\n",
      "Epoch [1/2], Step [332/500] Loss: 1.4175\n",
      "Epoch [1/2], Step [333/500] Loss: 1.1415\n",
      "Epoch [1/2], Step [334/500] Loss: 1.3912\n",
      "Epoch [1/2], Step [335/500] Loss: 1.2412\n",
      "Epoch [1/2], Step [336/500] Loss: 1.3581\n",
      "Epoch [1/2], Step [337/500] Loss: 1.4109\n",
      "Epoch [1/2], Step [338/500] Loss: 1.1443\n",
      "Epoch [1/2], Step [339/500] Loss: 1.3001\n",
      "Epoch [1/2], Step [340/500] Loss: 1.3646\n",
      "Epoch [1/2], Step [341/500] Loss: 1.1728\n",
      "Epoch [1/2], Step [342/500] Loss: 1.4122\n",
      "Epoch [1/2], Step [343/500] Loss: 1.5442\n",
      "Epoch [1/2], Step [344/500] Loss: 1.4372\n",
      "Epoch [1/2], Step [345/500] Loss: 1.3630\n",
      "Epoch [1/2], Step [346/500] Loss: 1.3856\n",
      "Epoch [1/2], Step [347/500] Loss: 1.3683\n",
      "Epoch [1/2], Step [348/500] Loss: 1.3416\n",
      "Epoch [1/2], Step [349/500] Loss: 1.3949\n",
      "Epoch [1/2], Step [350/500] Loss: 1.3739\n",
      "Epoch [1/2], Step [351/500] Loss: 1.3087\n",
      "Epoch [1/2], Step [352/500] Loss: 1.3424\n",
      "Epoch [1/2], Step [353/500] Loss: 1.2407\n",
      "Epoch [1/2], Step [354/500] Loss: 1.3287\n",
      "Epoch [1/2], Step [355/500] Loss: 1.2848\n",
      "Epoch [1/2], Step [356/500] Loss: 1.2813\n",
      "Epoch [1/2], Step [357/500] Loss: 1.1961\n",
      "Epoch [1/2], Step [358/500] Loss: 1.3346\n",
      "Epoch [1/2], Step [359/500] Loss: 1.2394\n",
      "Epoch [1/2], Step [360/500] Loss: 1.3248\n",
      "Epoch [1/2], Step [361/500] Loss: 1.1574\n",
      "Epoch [1/2], Step [362/500] Loss: 1.3219\n",
      "Epoch [1/2], Step [363/500] Loss: 1.2476\n",
      "Epoch [1/2], Step [364/500] Loss: 1.3417\n",
      "Epoch [1/2], Step [365/500] Loss: 1.3000\n",
      "Epoch [1/2], Step [366/500] Loss: 1.3733\n",
      "Epoch [1/2], Step [367/500] Loss: 1.3709\n",
      "Epoch [1/2], Step [368/500] Loss: 1.1908\n",
      "Epoch [1/2], Step [369/500] Loss: 1.3323\n",
      "Epoch [1/2], Step [370/500] Loss: 1.3732\n",
      "Epoch [1/2], Step [371/500] Loss: 1.3522\n",
      "Epoch [1/2], Step [372/500] Loss: 1.1859\n",
      "Epoch [1/2], Step [373/500] Loss: 1.2803\n",
      "Epoch [1/2], Step [374/500] Loss: 1.3733\n",
      "Epoch [1/2], Step [375/500] Loss: 1.3157\n",
      "Epoch [1/2], Step [376/500] Loss: 1.4096\n",
      "Epoch [1/2], Step [377/500] Loss: 1.3190\n",
      "Epoch [1/2], Step [378/500] Loss: 1.0342\n",
      "Epoch [1/2], Step [379/500] Loss: 1.3266\n",
      "Epoch [1/2], Step [380/500] Loss: 1.3845\n",
      "Epoch [1/2], Step [381/500] Loss: 1.2698\n",
      "Epoch [1/2], Step [382/500] Loss: 1.2550\n",
      "Epoch [1/2], Step [383/500] Loss: 1.1156\n",
      "Epoch [1/2], Step [384/500] Loss: 1.0962\n",
      "Epoch [1/2], Step [385/500] Loss: 1.1267\n",
      "Epoch [1/2], Step [386/500] Loss: 1.3854\n",
      "Epoch [1/2], Step [387/500] Loss: 1.3593\n",
      "Epoch [1/2], Step [388/500] Loss: 1.2726\n",
      "Epoch [1/2], Step [389/500] Loss: 1.2283\n",
      "Epoch [1/2], Step [390/500] Loss: 1.2445\n",
      "Epoch [1/2], Step [391/500] Loss: 1.3113\n",
      "Epoch [1/2], Step [392/500] Loss: 1.2398\n",
      "Epoch [1/2], Step [393/500] Loss: 1.2439\n",
      "Epoch [1/2], Step [394/500] Loss: 1.2192\n",
      "Epoch [1/2], Step [395/500] Loss: 1.3470\n",
      "Epoch [1/2], Step [396/500] Loss: 1.2298\n",
      "Epoch [1/2], Step [397/500] Loss: 1.1730\n",
      "Epoch [1/2], Step [398/500] Loss: 1.0910\n",
      "Epoch [1/2], Step [399/500] Loss: 1.2134\n",
      "Epoch [1/2], Step [400/500] Loss: 1.1799\n",
      "Epoch [1/2], Step [401/500] Loss: 1.2353\n",
      "Epoch [1/2], Step [402/500] Loss: 1.2975\n",
      "Epoch [1/2], Step [403/500] Loss: 1.2050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2], Step [404/500] Loss: 1.3463\n",
      "Epoch [1/2], Step [405/500] Loss: 1.2586\n",
      "Epoch [1/2], Step [406/500] Loss: 1.1480\n",
      "Epoch [1/2], Step [407/500] Loss: 1.2144\n",
      "Epoch [1/2], Step [408/500] Loss: 1.2044\n",
      "Epoch [1/2], Step [409/500] Loss: 1.2112\n",
      "Epoch [1/2], Step [410/500] Loss: 1.2650\n",
      "Epoch [1/2], Step [411/500] Loss: 1.2779\n",
      "Epoch [1/2], Step [412/500] Loss: 1.2561\n",
      "Epoch [1/2], Step [413/500] Loss: 1.1775\n",
      "Epoch [1/2], Step [414/500] Loss: 1.4571\n",
      "Epoch [1/2], Step [415/500] Loss: 1.4193\n",
      "Epoch [1/2], Step [416/500] Loss: 1.1280\n",
      "Epoch [1/2], Step [417/500] Loss: 1.2572\n",
      "Epoch [1/2], Step [418/500] Loss: 1.1814\n",
      "Epoch [1/2], Step [419/500] Loss: 1.1616\n",
      "Epoch [1/2], Step [420/500] Loss: 1.3687\n",
      "Epoch [1/2], Step [421/500] Loss: 1.1103\n",
      "Epoch [1/2], Step [422/500] Loss: 1.2243\n",
      "Epoch [1/2], Step [423/500] Loss: 1.2273\n",
      "Epoch [1/2], Step [424/500] Loss: 1.2811\n",
      "Epoch [1/2], Step [425/500] Loss: 1.2804\n",
      "Epoch [1/2], Step [426/500] Loss: 1.2095\n",
      "Epoch [1/2], Step [427/500] Loss: 1.1524\n",
      "Epoch [1/2], Step [428/500] Loss: 1.1858\n",
      "Epoch [1/2], Step [429/500] Loss: 1.1255\n",
      "Epoch [1/2], Step [430/500] Loss: 1.3080\n",
      "Epoch [1/2], Step [431/500] Loss: 1.1756\n",
      "Epoch [1/2], Step [432/500] Loss: 1.3561\n",
      "Epoch [1/2], Step [433/500] Loss: 1.0936\n",
      "Epoch [1/2], Step [434/500] Loss: 1.2331\n",
      "Epoch [1/2], Step [435/500] Loss: 1.2044\n",
      "Epoch [1/2], Step [436/500] Loss: 1.2122\n",
      "Epoch [1/2], Step [437/500] Loss: 1.1958\n",
      "Epoch [1/2], Step [438/500] Loss: 1.1181\n",
      "Epoch [1/2], Step [439/500] Loss: 1.3389\n",
      "Epoch [1/2], Step [440/500] Loss: 1.2590\n",
      "Epoch [1/2], Step [441/500] Loss: 1.2576\n",
      "Epoch [1/2], Step [442/500] Loss: 1.3575\n",
      "Epoch [1/2], Step [443/500] Loss: 1.1897\n",
      "Epoch [1/2], Step [444/500] Loss: 1.1479\n",
      "Epoch [1/2], Step [445/500] Loss: 1.2427\n",
      "Epoch [1/2], Step [446/500] Loss: 1.0754\n",
      "Epoch [1/2], Step [447/500] Loss: 1.1360\n",
      "Epoch [1/2], Step [448/500] Loss: 1.1516\n",
      "Epoch [1/2], Step [449/500] Loss: 1.1276\n",
      "Epoch [1/2], Step [450/500] Loss: 1.0138\n",
      "Epoch [1/2], Step [451/500] Loss: 1.1030\n",
      "Epoch [1/2], Step [452/500] Loss: 1.1770\n",
      "Epoch [1/2], Step [453/500] Loss: 1.2352\n",
      "Epoch [1/2], Step [454/500] Loss: 1.1935\n",
      "Epoch [1/2], Step [455/500] Loss: 1.3952\n",
      "Epoch [1/2], Step [456/500] Loss: 1.2162\n",
      "Epoch [1/2], Step [457/500] Loss: 1.1128\n",
      "Epoch [1/2], Step [458/500] Loss: 1.2216\n",
      "Epoch [1/2], Step [459/500] Loss: 1.0826\n",
      "Epoch [1/2], Step [460/500] Loss: 1.1990\n",
      "Epoch [1/2], Step [461/500] Loss: 1.3324\n",
      "Epoch [1/2], Step [462/500] Loss: 1.1802\n",
      "Epoch [1/2], Step [463/500] Loss: 1.1861\n",
      "Epoch [1/2], Step [464/500] Loss: 1.2123\n",
      "Epoch [1/2], Step [465/500] Loss: 1.1602\n",
      "Epoch [1/2], Step [466/500] Loss: 1.3269\n",
      "Epoch [1/2], Step [467/500] Loss: 1.2822\n",
      "Epoch [1/2], Step [468/500] Loss: 1.2082\n",
      "Epoch [1/2], Step [469/500] Loss: 1.1565\n",
      "Epoch [1/2], Step [470/500] Loss: 1.1364\n",
      "Epoch [1/2], Step [471/500] Loss: 1.1136\n",
      "Epoch [1/2], Step [472/500] Loss: 1.0280\n",
      "Epoch [1/2], Step [473/500] Loss: 1.0639\n",
      "Epoch [1/2], Step [474/500] Loss: 1.1141\n",
      "Epoch [1/2], Step [475/500] Loss: 1.2445\n",
      "Epoch [1/2], Step [476/500] Loss: 1.1897\n",
      "Epoch [1/2], Step [477/500] Loss: 1.0744\n",
      "Epoch [1/2], Step [478/500] Loss: 1.2009\n",
      "Epoch [1/2], Step [479/500] Loss: 1.1667\n",
      "Epoch [1/2], Step [480/500] Loss: 1.1762\n",
      "Epoch [1/2], Step [481/500] Loss: 1.2643\n",
      "Epoch [1/2], Step [482/500] Loss: 1.1476\n",
      "Epoch [1/2], Step [483/500] Loss: 1.2002\n",
      "Epoch [1/2], Step [484/500] Loss: 1.0908\n",
      "Epoch [1/2], Step [485/500] Loss: 1.0884\n",
      "Epoch [1/2], Step [486/500] Loss: 1.0994\n",
      "Epoch [1/2], Step [487/500] Loss: 1.1275\n",
      "Epoch [1/2], Step [488/500] Loss: 1.2605\n",
      "Epoch [1/2], Step [489/500] Loss: 1.1483\n",
      "Epoch [1/2], Step [490/500] Loss: 1.2214\n",
      "Epoch [1/2], Step [491/500] Loss: 1.4896\n",
      "Epoch [1/2], Step [492/500] Loss: 1.1147\n",
      "Epoch [1/2], Step [493/500] Loss: 1.0545\n",
      "Epoch [1/2], Step [494/500] Loss: 1.0767\n",
      "Epoch [1/2], Step [495/500] Loss: 1.0797\n",
      "Epoch [1/2], Step [496/500] Loss: 1.2469\n",
      "Epoch [1/2], Step [497/500] Loss: 0.9979\n",
      "Epoch [1/2], Step [498/500] Loss: 1.2005\n",
      "Epoch [1/2], Step [499/500] Loss: 1.0804\n",
      "Epoch [1/2], Step [500/500] Loss: 1.2163\n",
      "Epoch [2/2], Step [1/500] Loss: 1.1949\n",
      "Epoch [2/2], Step [2/500] Loss: 1.2223\n",
      "Epoch [2/2], Step [3/500] Loss: 1.1672\n",
      "Epoch [2/2], Step [4/500] Loss: 1.2414\n",
      "Epoch [2/2], Step [5/500] Loss: 1.0518\n",
      "Epoch [2/2], Step [6/500] Loss: 1.1514\n",
      "Epoch [2/2], Step [7/500] Loss: 1.1363\n",
      "Epoch [2/2], Step [8/500] Loss: 1.1097\n",
      "Epoch [2/2], Step [9/500] Loss: 1.2520\n",
      "Epoch [2/2], Step [10/500] Loss: 1.1731\n",
      "Epoch [2/2], Step [11/500] Loss: 1.2241\n",
      "Epoch [2/2], Step [12/500] Loss: 0.9388\n",
      "Epoch [2/2], Step [13/500] Loss: 1.1996\n",
      "Epoch [2/2], Step [14/500] Loss: 1.2486\n",
      "Epoch [2/2], Step [15/500] Loss: 1.1727\n",
      "Epoch [2/2], Step [16/500] Loss: 1.0639\n",
      "Epoch [2/2], Step [17/500] Loss: 1.1027\n",
      "Epoch [2/2], Step [18/500] Loss: 1.1723\n",
      "Epoch [2/2], Step [19/500] Loss: 1.0222\n",
      "Epoch [2/2], Step [20/500] Loss: 1.0749\n",
      "Epoch [2/2], Step [21/500] Loss: 1.1665\n",
      "Epoch [2/2], Step [22/500] Loss: 1.2301\n",
      "Epoch [2/2], Step [23/500] Loss: 0.9936\n",
      "Epoch [2/2], Step [24/500] Loss: 1.0653\n",
      "Epoch [2/2], Step [25/500] Loss: 1.3196\n",
      "Epoch [2/2], Step [26/500] Loss: 1.1878\n",
      "Epoch [2/2], Step [27/500] Loss: 1.0196\n",
      "Epoch [2/2], Step [28/500] Loss: 1.1309\n",
      "Epoch [2/2], Step [29/500] Loss: 1.2503\n",
      "Epoch [2/2], Step [30/500] Loss: 1.0716\n",
      "Epoch [2/2], Step [31/500] Loss: 1.1029\n",
      "Epoch [2/2], Step [32/500] Loss: 1.2598\n",
      "Epoch [2/2], Step [33/500] Loss: 1.1996\n",
      "Epoch [2/2], Step [34/500] Loss: 1.3289\n",
      "Epoch [2/2], Step [35/500] Loss: 1.0418\n",
      "Epoch [2/2], Step [36/500] Loss: 1.1614\n",
      "Epoch [2/2], Step [37/500] Loss: 1.2181\n",
      "Epoch [2/2], Step [38/500] Loss: 1.4149\n",
      "Epoch [2/2], Step [39/500] Loss: 1.2231\n",
      "Epoch [2/2], Step [40/500] Loss: 1.1717\n",
      "Epoch [2/2], Step [41/500] Loss: 1.1632\n",
      "Epoch [2/2], Step [42/500] Loss: 1.2512\n",
      "Epoch [2/2], Step [43/500] Loss: 1.0385\n",
      "Epoch [2/2], Step [44/500] Loss: 1.2424\n",
      "Epoch [2/2], Step [45/500] Loss: 1.0687\n",
      "Epoch [2/2], Step [46/500] Loss: 1.1458\n",
      "Epoch [2/2], Step [47/500] Loss: 1.0046\n",
      "Epoch [2/2], Step [48/500] Loss: 1.1019\n",
      "Epoch [2/2], Step [49/500] Loss: 1.0872\n",
      "Epoch [2/2], Step [50/500] Loss: 1.3880\n",
      "Epoch [2/2], Step [51/500] Loss: 1.0166\n",
      "Epoch [2/2], Step [52/500] Loss: 1.2326\n",
      "Epoch [2/2], Step [53/500] Loss: 1.3661\n",
      "Epoch [2/2], Step [54/500] Loss: 1.0750\n",
      "Epoch [2/2], Step [55/500] Loss: 1.0281\n",
      "Epoch [2/2], Step [56/500] Loss: 1.2751\n",
      "Epoch [2/2], Step [57/500] Loss: 1.1352\n",
      "Epoch [2/2], Step [58/500] Loss: 1.0583\n",
      "Epoch [2/2], Step [59/500] Loss: 1.2105\n",
      "Epoch [2/2], Step [60/500] Loss: 1.1060\n",
      "Epoch [2/2], Step [61/500] Loss: 1.2830\n",
      "Epoch [2/2], Step [62/500] Loss: 1.1770\n",
      "Epoch [2/2], Step [63/500] Loss: 1.1555\n",
      "Epoch [2/2], Step [64/500] Loss: 1.1732\n",
      "Epoch [2/2], Step [65/500] Loss: 1.1103\n",
      "Epoch [2/2], Step [66/500] Loss: 0.8940\n",
      "Epoch [2/2], Step [67/500] Loss: 1.1652\n",
      "Epoch [2/2], Step [68/500] Loss: 1.1406\n",
      "Epoch [2/2], Step [69/500] Loss: 1.2216\n",
      "Epoch [2/2], Step [70/500] Loss: 1.1656\n",
      "Epoch [2/2], Step [71/500] Loss: 1.1546\n",
      "Epoch [2/2], Step [72/500] Loss: 1.1828\n",
      "Epoch [2/2], Step [73/500] Loss: 1.1838\n",
      "Epoch [2/2], Step [74/500] Loss: 1.0497\n",
      "Epoch [2/2], Step [75/500] Loss: 1.1488\n",
      "Epoch [2/2], Step [76/500] Loss: 1.1290\n",
      "Epoch [2/2], Step [77/500] Loss: 1.1139\n",
      "Epoch [2/2], Step [78/500] Loss: 1.2287\n",
      "Epoch [2/2], Step [79/500] Loss: 1.1759\n",
      "Epoch [2/2], Step [80/500] Loss: 1.1335\n",
      "Epoch [2/2], Step [81/500] Loss: 1.2221\n",
      "Epoch [2/2], Step [82/500] Loss: 1.3031\n",
      "Epoch [2/2], Step [83/500] Loss: 1.1709\n",
      "Epoch [2/2], Step [84/500] Loss: 1.0858\n",
      "Epoch [2/2], Step [85/500] Loss: 1.1730\n",
      "Epoch [2/2], Step [86/500] Loss: 1.0765\n",
      "Epoch [2/2], Step [87/500] Loss: 1.2888\n",
      "Epoch [2/2], Step [88/500] Loss: 1.0725\n",
      "Epoch [2/2], Step [89/500] Loss: 1.0685\n",
      "Epoch [2/2], Step [90/500] Loss: 1.0843\n",
      "Epoch [2/2], Step [91/500] Loss: 1.0565\n",
      "Epoch [2/2], Step [92/500] Loss: 1.1578\n",
      "Epoch [2/2], Step [93/500] Loss: 1.0455\n",
      "Epoch [2/2], Step [94/500] Loss: 1.1460\n",
      "Epoch [2/2], Step [95/500] Loss: 1.2669\n",
      "Epoch [2/2], Step [96/500] Loss: 0.9240\n",
      "Epoch [2/2], Step [97/500] Loss: 1.0552\n",
      "Epoch [2/2], Step [98/500] Loss: 1.0073\n",
      "Epoch [2/2], Step [99/500] Loss: 1.1523\n",
      "Epoch [2/2], Step [100/500] Loss: 1.2110\n",
      "Epoch [2/2], Step [101/500] Loss: 1.0186\n",
      "Epoch [2/2], Step [102/500] Loss: 1.1704\n",
      "Epoch [2/2], Step [103/500] Loss: 1.0544\n",
      "Epoch [2/2], Step [104/500] Loss: 1.2906\n",
      "Epoch [2/2], Step [105/500] Loss: 1.1221\n",
      "Epoch [2/2], Step [106/500] Loss: 1.2411\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/2], Step [107/500] Loss: 1.1536\n",
      "Epoch [2/2], Step [108/500] Loss: 1.1177\n",
      "Epoch [2/2], Step [109/500] Loss: 1.0539\n",
      "Epoch [2/2], Step [110/500] Loss: 1.0847\n",
      "Epoch [2/2], Step [111/500] Loss: 1.1412\n",
      "Epoch [2/2], Step [112/500] Loss: 0.9852\n",
      "Epoch [2/2], Step [113/500] Loss: 1.1596\n",
      "Epoch [2/2], Step [114/500] Loss: 1.0379\n",
      "Epoch [2/2], Step [115/500] Loss: 1.1484\n",
      "Epoch [2/2], Step [116/500] Loss: 1.0701\n",
      "Epoch [2/2], Step [117/500] Loss: 1.0401\n",
      "Epoch [2/2], Step [118/500] Loss: 1.2463\n",
      "Epoch [2/2], Step [119/500] Loss: 0.9698\n",
      "Epoch [2/2], Step [120/500] Loss: 1.0563\n",
      "Epoch [2/2], Step [121/500] Loss: 0.9203\n",
      "Epoch [2/2], Step [122/500] Loss: 1.1829\n",
      "Epoch [2/2], Step [123/500] Loss: 1.0928\n",
      "Epoch [2/2], Step [124/500] Loss: 1.0534\n",
      "Epoch [2/2], Step [125/500] Loss: 1.0053\n",
      "Epoch [2/2], Step [126/500] Loss: 1.1918\n",
      "Epoch [2/2], Step [127/500] Loss: 1.2074\n",
      "Epoch [2/2], Step [128/500] Loss: 1.1324\n",
      "Epoch [2/2], Step [129/500] Loss: 1.0540\n",
      "Epoch [2/2], Step [130/500] Loss: 0.8369\n",
      "Epoch [2/2], Step [131/500] Loss: 1.2059\n",
      "Epoch [2/2], Step [132/500] Loss: 1.1358\n",
      "Epoch [2/2], Step [133/500] Loss: 1.1821\n",
      "Epoch [2/2], Step [134/500] Loss: 1.0001\n",
      "Epoch [2/2], Step [135/500] Loss: 1.2322\n",
      "Epoch [2/2], Step [136/500] Loss: 0.9861\n",
      "Epoch [2/2], Step [137/500] Loss: 0.9444\n",
      "Epoch [2/2], Step [138/500] Loss: 1.0720\n",
      "Epoch [2/2], Step [139/500] Loss: 1.1780\n",
      "Epoch [2/2], Step [140/500] Loss: 1.2682\n",
      "Epoch [2/2], Step [141/500] Loss: 0.9996\n",
      "Epoch [2/2], Step [142/500] Loss: 1.1870\n",
      "Epoch [2/2], Step [143/500] Loss: 0.9667\n",
      "Epoch [2/2], Step [144/500] Loss: 0.9596\n",
      "Epoch [2/2], Step [145/500] Loss: 1.1510\n",
      "Epoch [2/2], Step [146/500] Loss: 1.1243\n",
      "Epoch [2/2], Step [147/500] Loss: 1.1970\n",
      "Epoch [2/2], Step [148/500] Loss: 1.2925\n",
      "Epoch [2/2], Step [149/500] Loss: 1.0764\n",
      "Epoch [2/2], Step [150/500] Loss: 0.9378\n",
      "Epoch [2/2], Step [151/500] Loss: 1.0709\n",
      "Epoch [2/2], Step [152/500] Loss: 1.0913\n",
      "Epoch [2/2], Step [153/500] Loss: 1.2276\n",
      "Epoch [2/2], Step [154/500] Loss: 1.0763\n",
      "Epoch [2/2], Step [155/500] Loss: 1.1081\n",
      "Epoch [2/2], Step [156/500] Loss: 1.1215\n",
      "Epoch [2/2], Step [157/500] Loss: 1.3584\n",
      "Epoch [2/2], Step [158/500] Loss: 1.0610\n",
      "Epoch [2/2], Step [159/500] Loss: 0.9067\n",
      "Epoch [2/2], Step [160/500] Loss: 1.3142\n",
      "Epoch [2/2], Step [161/500] Loss: 1.1862\n",
      "Epoch [2/2], Step [162/500] Loss: 0.9715\n",
      "Epoch [2/2], Step [163/500] Loss: 1.2674\n",
      "Epoch [2/2], Step [164/500] Loss: 1.2437\n",
      "Epoch [2/2], Step [165/500] Loss: 1.0380\n",
      "Epoch [2/2], Step [166/500] Loss: 1.1414\n",
      "Epoch [2/2], Step [167/500] Loss: 1.2234\n",
      "Epoch [2/2], Step [168/500] Loss: 1.0756\n",
      "Epoch [2/2], Step [169/500] Loss: 1.1082\n",
      "Epoch [2/2], Step [170/500] Loss: 1.0056\n",
      "Epoch [2/2], Step [171/500] Loss: 1.0520\n",
      "Epoch [2/2], Step [172/500] Loss: 1.0435\n",
      "Epoch [2/2], Step [173/500] Loss: 1.1155\n",
      "Epoch [2/2], Step [174/500] Loss: 1.0230\n",
      "Epoch [2/2], Step [175/500] Loss: 1.0249\n",
      "Epoch [2/2], Step [176/500] Loss: 1.1725\n",
      "Epoch [2/2], Step [177/500] Loss: 1.3496\n",
      "Epoch [2/2], Step [178/500] Loss: 1.0629\n",
      "Epoch [2/2], Step [179/500] Loss: 0.9956\n",
      "Epoch [2/2], Step [180/500] Loss: 1.0021\n",
      "Epoch [2/2], Step [181/500] Loss: 1.2528\n",
      "Epoch [2/2], Step [182/500] Loss: 1.1234\n",
      "Epoch [2/2], Step [183/500] Loss: 1.0817\n",
      "Epoch [2/2], Step [184/500] Loss: 1.0119\n",
      "Epoch [2/2], Step [185/500] Loss: 0.9446\n",
      "Epoch [2/2], Step [186/500] Loss: 1.2044\n",
      "Epoch [2/2], Step [187/500] Loss: 0.9388\n",
      "Epoch [2/2], Step [188/500] Loss: 1.0269\n",
      "Epoch [2/2], Step [189/500] Loss: 0.9917\n",
      "Epoch [2/2], Step [190/500] Loss: 1.0036\n",
      "Epoch [2/2], Step [191/500] Loss: 1.1017\n",
      "Epoch [2/2], Step [192/500] Loss: 1.1680\n",
      "Epoch [2/2], Step [193/500] Loss: 1.0723\n",
      "Epoch [2/2], Step [194/500] Loss: 0.9455\n",
      "Epoch [2/2], Step [195/500] Loss: 1.0946\n",
      "Epoch [2/2], Step [196/500] Loss: 1.0388\n",
      "Epoch [2/2], Step [197/500] Loss: 1.1700\n",
      "Epoch [2/2], Step [198/500] Loss: 1.2479\n",
      "Epoch [2/2], Step [199/500] Loss: 1.0611\n",
      "Epoch [2/2], Step [200/500] Loss: 1.1348\n",
      "Epoch [2/2], Step [201/500] Loss: 1.1357\n",
      "Epoch [2/2], Step [202/500] Loss: 1.0359\n",
      "Epoch [2/2], Step [203/500] Loss: 1.3692\n",
      "Epoch [2/2], Step [204/500] Loss: 0.9521\n",
      "Epoch [2/2], Step [205/500] Loss: 0.9298\n",
      "Epoch [2/2], Step [206/500] Loss: 1.0904\n",
      "Epoch [2/2], Step [207/500] Loss: 1.1488\n",
      "Epoch [2/2], Step [208/500] Loss: 1.0547\n",
      "Epoch [2/2], Step [209/500] Loss: 1.1118\n",
      "Epoch [2/2], Step [210/500] Loss: 1.0822\n",
      "Epoch [2/2], Step [211/500] Loss: 1.1267\n",
      "Epoch [2/2], Step [212/500] Loss: 1.0163\n",
      "Epoch [2/2], Step [213/500] Loss: 1.1322\n",
      "Epoch [2/2], Step [214/500] Loss: 1.0700\n",
      "Epoch [2/2], Step [215/500] Loss: 1.0362\n",
      "Epoch [2/2], Step [216/500] Loss: 1.1719\n",
      "Epoch [2/2], Step [217/500] Loss: 1.1299\n",
      "Epoch [2/2], Step [218/500] Loss: 1.2380\n",
      "Epoch [2/2], Step [219/500] Loss: 0.9924\n",
      "Epoch [2/2], Step [220/500] Loss: 1.0947\n",
      "Epoch [2/2], Step [221/500] Loss: 1.1404\n",
      "Epoch [2/2], Step [222/500] Loss: 0.9919\n",
      "Epoch [2/2], Step [223/500] Loss: 1.0118\n",
      "Epoch [2/2], Step [224/500] Loss: 1.0577\n",
      "Epoch [2/2], Step [225/500] Loss: 1.0783\n",
      "Epoch [2/2], Step [226/500] Loss: 0.9136\n",
      "Epoch [2/2], Step [227/500] Loss: 1.0431\n",
      "Epoch [2/2], Step [228/500] Loss: 1.1251\n",
      "Epoch [2/2], Step [229/500] Loss: 1.0394\n",
      "Epoch [2/2], Step [230/500] Loss: 1.0024\n",
      "Epoch [2/2], Step [231/500] Loss: 1.1019\n",
      "Epoch [2/2], Step [232/500] Loss: 0.9113\n",
      "Epoch [2/2], Step [233/500] Loss: 1.0341\n",
      "Epoch [2/2], Step [234/500] Loss: 1.3768\n",
      "Epoch [2/2], Step [235/500] Loss: 1.1946\n",
      "Epoch [2/2], Step [236/500] Loss: 1.1028\n",
      "Epoch [2/2], Step [237/500] Loss: 1.0259\n",
      "Epoch [2/2], Step [238/500] Loss: 0.9615\n",
      "Epoch [2/2], Step [239/500] Loss: 0.9905\n",
      "Epoch [2/2], Step [240/500] Loss: 0.9710\n",
      "Epoch [2/2], Step [241/500] Loss: 1.0777\n",
      "Epoch [2/2], Step [242/500] Loss: 1.1611\n",
      "Epoch [2/2], Step [243/500] Loss: 1.0654\n",
      "Epoch [2/2], Step [244/500] Loss: 0.9477\n",
      "Epoch [2/2], Step [245/500] Loss: 1.1437\n",
      "Epoch [2/2], Step [246/500] Loss: 0.9383\n",
      "Epoch [2/2], Step [247/500] Loss: 1.1660\n",
      "Epoch [2/2], Step [248/500] Loss: 0.8611\n",
      "Epoch [2/2], Step [249/500] Loss: 0.9284\n",
      "Epoch [2/2], Step [250/500] Loss: 1.0852\n",
      "Epoch [2/2], Step [251/500] Loss: 0.9378\n",
      "Epoch [2/2], Step [252/500] Loss: 1.0029\n",
      "Epoch [2/2], Step [253/500] Loss: 1.1081\n",
      "Epoch [2/2], Step [254/500] Loss: 0.9518\n",
      "Epoch [2/2], Step [255/500] Loss: 1.0394\n",
      "Epoch [2/2], Step [256/500] Loss: 1.1102\n",
      "Epoch [2/2], Step [257/500] Loss: 0.9539\n",
      "Epoch [2/2], Step [258/500] Loss: 0.9912\n",
      "Epoch [2/2], Step [259/500] Loss: 0.9092\n",
      "Epoch [2/2], Step [260/500] Loss: 1.0003\n",
      "Epoch [2/2], Step [261/500] Loss: 1.1818\n",
      "Epoch [2/2], Step [262/500] Loss: 1.0256\n",
      "Epoch [2/2], Step [263/500] Loss: 1.0919\n",
      "Epoch [2/2], Step [264/500] Loss: 1.0259\n",
      "Epoch [2/2], Step [265/500] Loss: 0.9504\n",
      "Epoch [2/2], Step [266/500] Loss: 1.0481\n",
      "Epoch [2/2], Step [267/500] Loss: 1.0226\n",
      "Epoch [2/2], Step [268/500] Loss: 1.1474\n",
      "Epoch [2/2], Step [269/500] Loss: 1.1935\n",
      "Epoch [2/2], Step [270/500] Loss: 1.0515\n",
      "Epoch [2/2], Step [271/500] Loss: 1.0109\n",
      "Epoch [2/2], Step [272/500] Loss: 1.1364\n",
      "Epoch [2/2], Step [273/500] Loss: 1.1508\n",
      "Epoch [2/2], Step [274/500] Loss: 1.1109\n",
      "Epoch [2/2], Step [275/500] Loss: 1.0247\n",
      "Epoch [2/2], Step [276/500] Loss: 0.9662\n",
      "Epoch [2/2], Step [277/500] Loss: 1.2880\n",
      "Epoch [2/2], Step [278/500] Loss: 1.0069\n",
      "Epoch [2/2], Step [279/500] Loss: 1.1922\n",
      "Epoch [2/2], Step [280/500] Loss: 1.2139\n",
      "Epoch [2/2], Step [281/500] Loss: 1.1154\n",
      "Epoch [2/2], Step [282/500] Loss: 1.0185\n",
      "Epoch [2/2], Step [283/500] Loss: 0.9893\n",
      "Epoch [2/2], Step [284/500] Loss: 1.0848\n",
      "Epoch [2/2], Step [285/500] Loss: 0.9145\n",
      "Epoch [2/2], Step [286/500] Loss: 1.0029\n",
      "Epoch [2/2], Step [287/500] Loss: 1.1601\n",
      "Epoch [2/2], Step [288/500] Loss: 1.0738\n",
      "Epoch [2/2], Step [289/500] Loss: 1.1618\n",
      "Epoch [2/2], Step [290/500] Loss: 1.0996\n",
      "Epoch [2/2], Step [291/500] Loss: 1.1847\n",
      "Epoch [2/2], Step [292/500] Loss: 1.0620\n",
      "Epoch [2/2], Step [293/500] Loss: 0.9409\n",
      "Epoch [2/2], Step [294/500] Loss: 1.2962\n",
      "Epoch [2/2], Step [295/500] Loss: 0.7931\n",
      "Epoch [2/2], Step [296/500] Loss: 1.3615\n",
      "Epoch [2/2], Step [297/500] Loss: 0.8445\n",
      "Epoch [2/2], Step [298/500] Loss: 1.0621\n",
      "Epoch [2/2], Step [299/500] Loss: 0.9332\n",
      "Epoch [2/2], Step [300/500] Loss: 1.0072\n",
      "Epoch [2/2], Step [301/500] Loss: 1.0713\n",
      "Epoch [2/2], Step [302/500] Loss: 0.8884\n",
      "Epoch [2/2], Step [303/500] Loss: 1.1225\n",
      "Epoch [2/2], Step [304/500] Loss: 1.1130\n",
      "Epoch [2/2], Step [305/500] Loss: 1.0399\n",
      "Epoch [2/2], Step [306/500] Loss: 0.9906\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/2], Step [307/500] Loss: 1.1078\n",
      "Epoch [2/2], Step [308/500] Loss: 1.1141\n",
      "Epoch [2/2], Step [309/500] Loss: 1.0154\n",
      "Epoch [2/2], Step [310/500] Loss: 0.9645\n",
      "Epoch [2/2], Step [311/500] Loss: 1.0366\n",
      "Epoch [2/2], Step [312/500] Loss: 0.9836\n",
      "Epoch [2/2], Step [313/500] Loss: 1.0035\n",
      "Epoch [2/2], Step [314/500] Loss: 0.9024\n",
      "Epoch [2/2], Step [315/500] Loss: 1.1951\n",
      "Epoch [2/2], Step [316/500] Loss: 0.9812\n",
      "Epoch [2/2], Step [317/500] Loss: 1.0483\n",
      "Epoch [2/2], Step [318/500] Loss: 0.8998\n",
      "Epoch [2/2], Step [319/500] Loss: 0.9192\n",
      "Epoch [2/2], Step [320/500] Loss: 1.0050\n",
      "Epoch [2/2], Step [321/500] Loss: 1.1316\n",
      "Epoch [2/2], Step [322/500] Loss: 0.8479\n",
      "Epoch [2/2], Step [323/500] Loss: 1.0059\n",
      "Epoch [2/2], Step [324/500] Loss: 1.0367\n",
      "Epoch [2/2], Step [325/500] Loss: 0.8875\n",
      "Epoch [2/2], Step [326/500] Loss: 1.2369\n",
      "Epoch [2/2], Step [327/500] Loss: 1.0009\n",
      "Epoch [2/2], Step [328/500] Loss: 0.8089\n",
      "Epoch [2/2], Step [329/500] Loss: 0.9664\n",
      "Epoch [2/2], Step [330/500] Loss: 0.8948\n",
      "Epoch [2/2], Step [331/500] Loss: 0.9359\n",
      "Epoch [2/2], Step [332/500] Loss: 1.0029\n",
      "Epoch [2/2], Step [333/500] Loss: 1.0722\n",
      "Epoch [2/2], Step [334/500] Loss: 0.9420\n",
      "Epoch [2/2], Step [335/500] Loss: 0.9728\n",
      "Epoch [2/2], Step [336/500] Loss: 1.0868\n",
      "Epoch [2/2], Step [337/500] Loss: 1.1230\n",
      "Epoch [2/2], Step [338/500] Loss: 0.9256\n",
      "Epoch [2/2], Step [339/500] Loss: 1.1116\n",
      "Epoch [2/2], Step [340/500] Loss: 1.2351\n",
      "Epoch [2/2], Step [341/500] Loss: 1.0163\n",
      "Epoch [2/2], Step [342/500] Loss: 1.0433\n",
      "Epoch [2/2], Step [343/500] Loss: 1.1825\n",
      "Epoch [2/2], Step [344/500] Loss: 1.0169\n",
      "Epoch [2/2], Step [345/500] Loss: 0.9602\n",
      "Epoch [2/2], Step [346/500] Loss: 1.0141\n",
      "Epoch [2/2], Step [347/500] Loss: 1.0501\n",
      "Epoch [2/2], Step [348/500] Loss: 1.0065\n",
      "Epoch [2/2], Step [349/500] Loss: 0.9460\n",
      "Epoch [2/2], Step [350/500] Loss: 1.0720\n",
      "Epoch [2/2], Step [351/500] Loss: 1.0218\n",
      "Epoch [2/2], Step [352/500] Loss: 1.0987\n",
      "Epoch [2/2], Step [353/500] Loss: 1.1415\n",
      "Epoch [2/2], Step [354/500] Loss: 0.8054\n",
      "Epoch [2/2], Step [355/500] Loss: 0.8457\n",
      "Epoch [2/2], Step [356/500] Loss: 1.0491\n",
      "Epoch [2/2], Step [357/500] Loss: 0.9751\n",
      "Epoch [2/2], Step [358/500] Loss: 1.1120\n",
      "Epoch [2/2], Step [359/500] Loss: 1.1644\n",
      "Epoch [2/2], Step [360/500] Loss: 0.9188\n",
      "Epoch [2/2], Step [361/500] Loss: 1.0390\n",
      "Epoch [2/2], Step [362/500] Loss: 1.0009\n",
      "Epoch [2/2], Step [363/500] Loss: 0.9510\n",
      "Epoch [2/2], Step [364/500] Loss: 0.9705\n",
      "Epoch [2/2], Step [365/500] Loss: 0.9526\n",
      "Epoch [2/2], Step [366/500] Loss: 1.0324\n",
      "Epoch [2/2], Step [367/500] Loss: 0.9349\n",
      "Epoch [2/2], Step [368/500] Loss: 0.9921\n",
      "Epoch [2/2], Step [369/500] Loss: 0.9169\n",
      "Epoch [2/2], Step [370/500] Loss: 1.0205\n",
      "Epoch [2/2], Step [371/500] Loss: 1.1547\n",
      "Epoch [2/2], Step [372/500] Loss: 1.0158\n",
      "Epoch [2/2], Step [373/500] Loss: 1.0456\n",
      "Epoch [2/2], Step [374/500] Loss: 1.1702\n",
      "Epoch [2/2], Step [375/500] Loss: 0.9744\n",
      "Epoch [2/2], Step [376/500] Loss: 1.0187\n",
      "Epoch [2/2], Step [377/500] Loss: 0.9626\n",
      "Epoch [2/2], Step [378/500] Loss: 1.2907\n",
      "Epoch [2/2], Step [379/500] Loss: 1.0243\n",
      "Epoch [2/2], Step [380/500] Loss: 1.0474\n",
      "Epoch [2/2], Step [381/500] Loss: 1.0802\n",
      "Epoch [2/2], Step [382/500] Loss: 0.8212\n",
      "Epoch [2/2], Step [383/500] Loss: 1.0213\n",
      "Epoch [2/2], Step [384/500] Loss: 1.0731\n",
      "Epoch [2/2], Step [385/500] Loss: 0.9622\n",
      "Epoch [2/2], Step [386/500] Loss: 0.9515\n",
      "Epoch [2/2], Step [387/500] Loss: 1.0470\n",
      "Epoch [2/2], Step [388/500] Loss: 1.0837\n",
      "Epoch [2/2], Step [389/500] Loss: 0.9485\n",
      "Epoch [2/2], Step [390/500] Loss: 1.0981\n",
      "Epoch [2/2], Step [391/500] Loss: 0.8921\n",
      "Epoch [2/2], Step [392/500] Loss: 0.8011\n",
      "Epoch [2/2], Step [393/500] Loss: 1.2266\n",
      "Epoch [2/2], Step [394/500] Loss: 1.0373\n",
      "Epoch [2/2], Step [395/500] Loss: 1.0336\n",
      "Epoch [2/2], Step [396/500] Loss: 0.9566\n",
      "Epoch [2/2], Step [397/500] Loss: 1.0898\n",
      "Epoch [2/2], Step [398/500] Loss: 1.0527\n",
      "Epoch [2/2], Step [399/500] Loss: 1.1221\n",
      "Epoch [2/2], Step [400/500] Loss: 0.9675\n",
      "Epoch [2/2], Step [401/500] Loss: 1.1257\n",
      "Epoch [2/2], Step [402/500] Loss: 0.8971\n",
      "Epoch [2/2], Step [403/500] Loss: 1.0253\n",
      "Epoch [2/2], Step [404/500] Loss: 0.8592\n",
      "Epoch [2/2], Step [405/500] Loss: 0.9768\n",
      "Epoch [2/2], Step [406/500] Loss: 0.9220\n",
      "Epoch [2/2], Step [407/500] Loss: 1.0436\n",
      "Epoch [2/2], Step [408/500] Loss: 0.8586\n",
      "Epoch [2/2], Step [409/500] Loss: 1.0545\n",
      "Epoch [2/2], Step [410/500] Loss: 1.0973\n",
      "Epoch [2/2], Step [411/500] Loss: 1.0039\n",
      "Epoch [2/2], Step [412/500] Loss: 0.8547\n",
      "Epoch [2/2], Step [413/500] Loss: 0.8458\n",
      "Epoch [2/2], Step [414/500] Loss: 0.9263\n",
      "Epoch [2/2], Step [415/500] Loss: 1.0226\n",
      "Epoch [2/2], Step [416/500] Loss: 1.0286\n",
      "Epoch [2/2], Step [417/500] Loss: 1.0811\n",
      "Epoch [2/2], Step [418/500] Loss: 1.0446\n",
      "Epoch [2/2], Step [419/500] Loss: 1.0120\n",
      "Epoch [2/2], Step [420/500] Loss: 1.0354\n",
      "Epoch [2/2], Step [421/500] Loss: 1.3761\n",
      "Epoch [2/2], Step [422/500] Loss: 0.8590\n",
      "Epoch [2/2], Step [423/500] Loss: 0.8081\n",
      "Epoch [2/2], Step [424/500] Loss: 0.9174\n",
      "Epoch [2/2], Step [425/500] Loss: 0.8976\n",
      "Epoch [2/2], Step [426/500] Loss: 0.7977\n",
      "Epoch [2/2], Step [427/500] Loss: 0.8086\n",
      "Epoch [2/2], Step [428/500] Loss: 0.9393\n",
      "Epoch [2/2], Step [429/500] Loss: 1.0194\n",
      "Epoch [2/2], Step [430/500] Loss: 1.0322\n",
      "Epoch [2/2], Step [431/500] Loss: 1.0801\n",
      "Epoch [2/2], Step [432/500] Loss: 1.1568\n",
      "Epoch [2/2], Step [433/500] Loss: 1.0184\n",
      "Epoch [2/2], Step [434/500] Loss: 1.1355\n",
      "Epoch [2/2], Step [435/500] Loss: 0.8616\n",
      "Epoch [2/2], Step [436/500] Loss: 0.8242\n",
      "Epoch [2/2], Step [437/500] Loss: 0.9708\n",
      "Epoch [2/2], Step [438/500] Loss: 0.9821\n",
      "Epoch [2/2], Step [439/500] Loss: 0.9362\n",
      "Epoch [2/2], Step [440/500] Loss: 1.0406\n",
      "Epoch [2/2], Step [441/500] Loss: 1.2056\n",
      "Epoch [2/2], Step [442/500] Loss: 1.2043\n",
      "Epoch [2/2], Step [443/500] Loss: 1.0504\n",
      "Epoch [2/2], Step [444/500] Loss: 0.9704\n",
      "Epoch [2/2], Step [445/500] Loss: 0.9582\n",
      "Epoch [2/2], Step [446/500] Loss: 1.0118\n",
      "Epoch [2/2], Step [447/500] Loss: 1.0643\n",
      "Epoch [2/2], Step [448/500] Loss: 1.0720\n",
      "Epoch [2/2], Step [449/500] Loss: 1.0424\n",
      "Epoch [2/2], Step [450/500] Loss: 1.1739\n",
      "Epoch [2/2], Step [451/500] Loss: 0.8666\n",
      "Epoch [2/2], Step [452/500] Loss: 0.9405\n",
      "Epoch [2/2], Step [453/500] Loss: 0.9449\n",
      "Epoch [2/2], Step [454/500] Loss: 0.8716\n",
      "Epoch [2/2], Step [455/500] Loss: 0.9170\n",
      "Epoch [2/2], Step [456/500] Loss: 1.0809\n",
      "Epoch [2/2], Step [457/500] Loss: 0.9854\n",
      "Epoch [2/2], Step [458/500] Loss: 1.1086\n",
      "Epoch [2/2], Step [459/500] Loss: 1.1478\n",
      "Epoch [2/2], Step [460/500] Loss: 1.0035\n",
      "Epoch [2/2], Step [461/500] Loss: 1.0529\n",
      "Epoch [2/2], Step [462/500] Loss: 0.9932\n",
      "Epoch [2/2], Step [463/500] Loss: 0.9627\n",
      "Epoch [2/2], Step [464/500] Loss: 1.1880\n",
      "Epoch [2/2], Step [465/500] Loss: 1.0345\n",
      "Epoch [2/2], Step [466/500] Loss: 1.1920\n",
      "Epoch [2/2], Step [467/500] Loss: 0.9754\n",
      "Epoch [2/2], Step [468/500] Loss: 1.1078\n",
      "Epoch [2/2], Step [469/500] Loss: 0.9975\n",
      "Epoch [2/2], Step [470/500] Loss: 0.9539\n",
      "Epoch [2/2], Step [471/500] Loss: 0.9487\n",
      "Epoch [2/2], Step [472/500] Loss: 0.9204\n",
      "Epoch [2/2], Step [473/500] Loss: 0.9420\n",
      "Epoch [2/2], Step [474/500] Loss: 1.0417\n",
      "Epoch [2/2], Step [475/500] Loss: 0.8524\n",
      "Epoch [2/2], Step [476/500] Loss: 0.9114\n",
      "Epoch [2/2], Step [477/500] Loss: 0.9560\n",
      "Epoch [2/2], Step [478/500] Loss: 0.9395\n",
      "Epoch [2/2], Step [479/500] Loss: 0.9046\n",
      "Epoch [2/2], Step [480/500] Loss: 1.1188\n",
      "Epoch [2/2], Step [481/500] Loss: 1.0748\n",
      "Epoch [2/2], Step [482/500] Loss: 0.9988\n",
      "Epoch [2/2], Step [483/500] Loss: 0.9151\n",
      "Epoch [2/2], Step [484/500] Loss: 0.7849\n",
      "Epoch [2/2], Step [485/500] Loss: 0.8959\n",
      "Epoch [2/2], Step [486/500] Loss: 0.9649\n",
      "Epoch [2/2], Step [487/500] Loss: 0.9353\n",
      "Epoch [2/2], Step [488/500] Loss: 0.8307\n",
      "Epoch [2/2], Step [489/500] Loss: 1.1390\n",
      "Epoch [2/2], Step [490/500] Loss: 1.1839\n",
      "Epoch [2/2], Step [491/500] Loss: 1.0602\n",
      "Epoch [2/2], Step [492/500] Loss: 1.2219\n",
      "Epoch [2/2], Step [493/500] Loss: 0.8230\n",
      "Epoch [2/2], Step [494/500] Loss: 0.9557\n",
      "Epoch [2/2], Step [495/500] Loss: 0.8673\n",
      "Epoch [2/2], Step [496/500] Loss: 0.9881\n",
      "Epoch [2/2], Step [497/500] Loss: 1.0916\n",
      "Epoch [2/2], Step [498/500] Loss: 0.8511\n",
      "Epoch [2/2], Step [499/500] Loss: 0.7921\n",
      "Epoch [2/2], Step [500/500] Loss: 1.0594\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i,(images, labels) in enumerate(train_loader):\n",
    "        # get OUTPUT & GT\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward through net & loss(output, labels)\n",
    "        outputs = model(images)\n",
    "        #print(labels)\n",
    "        #print(outputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # zero & backward & step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        # logging\n",
    "        if (i+1) % 1 == 0:\n",
    "            print (\"Epoch [{}/{}], Step [{}/{}] Loss: {:.4f}\".format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "    # Decay learning rate\n",
    "    if (epoch+1) % 20 == 0:\n",
    "        curr_lr /= 3\n",
    "        update_lr(optimizer, curr_lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/100]Accuracy of the model on the test images: 11.3000 %\n",
      "[20/100]Accuracy of the model on the test images: 10.6500 %\n",
      "[30/100]Accuracy of the model on the test images: 11.1333 %\n",
      "[40/100]Accuracy of the model on the test images: 10.5000 %\n",
      "[50/100]Accuracy of the model on the test images: 10.5000 %\n",
      "[60/100]Accuracy of the model on the test images: 10.2500 %\n",
      "[70/100]Accuracy of the model on the test images: 10.0286 %\n",
      "[80/100]Accuracy of the model on the test images: 10.1125 %\n",
      "[90/100]Accuracy of the model on the test images: 9.9444 %\n",
      "[100/100]Accuracy of the model on the test images: 9.9000 %\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct =0\n",
    "    total = 0\n",
    "    total_step = len(test_loader)\n",
    "    for i,(images, lebels) in enumerate(test_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        if (i+1) % 10 == 0:\n",
    "            print('[{}/{}]Accuracy of the model on the test images: {:.4f} %'.format(i+1, total_step, 100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
